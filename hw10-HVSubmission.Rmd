---
title: "CSCI E-63C Week 10 assignment"
output:
  html_document:
    toc: true
---
Student: Huynh Vo
```{r setup, include=FALSE}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
library(xtable)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this assignment we will compare performance of random forest to that of LDA and KNN on a simulated dataset where we know exactly what is the association between predictors and outcome.  The relationship between predictor levels and the outcome will involve interaction that is notoriously difficult to model by methods such as LDA. The following example below illustrates the main ideas on a 3D dataset with two of the three attributes associated with the outcome:

```{r}
# How many observations:
nObs <- 1000
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 1
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate dataset with interaction between attribute levels associated with the outcome:
xyzTmp <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
classTmp <- 1
for ( iTmp in 1:nClassVars ) {
  deltaTmp <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
  xyzTmp[,iTmp] <- xyzTmp[,iTmp] + deltaTmp
  classTmp <- classTmp * deltaTmp
}
classTmp <- factor(classTmp > 0)
table(classTmp)
# plot resulting attribute levels colored by outcome:
pairs(xyzTmp,col=as.numeric(classTmp))
```


We can see that it is the interaction between the first two variables that has influences the outcome (we simulated it this way, of course!) and that points belonging to each of the two classes cannot be readily separated by a single line in 2D (or a single surface in 3D).

```{r}
# Split data into training dataset and testing dataset:
bTrain <- sample(c(FALSE,TRUE),nrow(xyzTmp),replace=TRUE)
# Fit random forest to train data, obtain test error:
rfRes <- randomForest(xyzTmp[bTrain,],classTmp[bTrain])
rfTmpTbl <- table(classTmp[!bTrain],predict(rfRes,newdata=xyzTmp[!bTrain,]))
rfTmpTbl
```


Random forest seems to do reasonably well on such dataset.

```{r}
# Fit LDA model to train data and evaluate error on the test data:
ldaRes <- lda(xyzTmp[bTrain,],classTmp[bTrain])
ldaTmpTbl <- table(classTmp[!bTrain],predict(ldaRes,newdata=xyzTmp[!bTrain,])$class)
ldaTmpTbl
```


LDA, on the other hand, not so good! (not a surprise given what we've seen above).  What about a more flexible method such a KNN?  Let's check it out remembering that k -- number of neihbors -- in KNN is the parameter to modulate its flexibility (i.e. bias-variance tradeoff).

```{r}
# Fit KNN model at several levels of k:
dfTmp <- NULL
for ( kTmp in sort(unique(floor(1.2^(1:33)))) ) {
  knnRes <- knn(xyzTmp[bTrain,],xyzTmp[!bTrain,],classTmp[bTrain],k=kTmp)
  tmpTbl <- table(classTmp[!bTrain],knnRes)
  dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
```


We can see from the above that there is a range of $k$ values where test error of KNN is the lowest and it is even lower that that of RF.  Now would be a good moment to think why one would want to choose RF over KNN or vice a versa for modeling the data if the figure above was representative of their true relative performance on a new dataset.

For the purposes of the assignment you can use the code above (probably best to wrap reusable parts of it into function(s)) to generate data with varying numbers of predictors associated with outcome and not, different numbers of observations and differences in the average values of predictors' between two classes as required below. These differences between datasets and parameters of the call to random forest will illustrate some of the factors influencing relative performance of random forest, LDA and KNN classifiers.  When comparing to KNN performance, please choose value(s) of `k` such that it performs sufficiently well -- feel free to refer to the plot above to select useful value(s) of `k` that you would like to evaluate here.  Keep in mind also that the value of `k` cannot be larger than the number of observations in the training dataset.

# Sub-problem 1 (15 points): effect of sample size

Generate training datasets with `nObs=25`, `100` and `500` observations such that two variables are associated with the outcome as parameterized above and three are not associated and average difference between the two classes is the same as above (i.e. in the notation from the above code `nClassVars=2`, `nNoiseVars=3` and `deltaClass=1`).  Obtain random forest, LDA and KNN test error rates on a (for greater stability of the results, much larger, say, with 10K observations) test dataset simulated from the same model.  Describe the differences between different methods and across the sample sizes used here.
Answer:
Generate testing datasets with nObs=10000:
```{r}
# How many observations:
nObs2 <- 10000
# How many predictors are associated with outcome:
nClassVars2 <- 2
# How many predictors are not:
nNoiseVars2 <- 3
# To modulate average difference between two classes' predictor values:
deltaClass2 <- 1
# Simulate dataset with interaction between attribute levels associated with the outcome:
xyzTmp2 <- matrix(rnorm(nObs2*(nClassVars2+nNoiseVars2)),nrow=nObs2,ncol=nClassVars2+nNoiseVars2)
classTmp2 <- 1
for ( iTmp in 1:nClassVars2 ) {
  deltaTmp2 <- sample(deltaClass2*c(-1,1),nObs2,replace=TRUE)
  xyzTmp2[,iTmp] <- xyzTmp2[,iTmp] + deltaTmp2
  classTmp2 <- classTmp2 * deltaTmp2
}
classTmp2 <- factor(classTmp2 > 0)
table(classTmp2)
# plot resulting attribute levels colored by outcome:
pairs(xyzTmp2,col=as.numeric(classTmp2))
```


Generate training datasets with nObs=25, 100 and 500:
```{r}
# How many observations:
nObs1 <- c(25, 100, 500)
# How many predictors are associated with outcome:
nClassVars1 <- 2
# How many predictors are not:
nNoiseVars1 <- 3
# To modulate average difference between two classes' predictor values:
deltaClass1 <- 1
# Simulate dataset with interaction between attribute levels associated with the outcome:

for (iObs in nObs1) {
  xyzTmp1 <- matrix(rnorm(iObs*(nClassVars1+nNoiseVars1)),nrow=iObs,ncol=nClassVars1+nNoiseVars1)
  classTmp1 <- 1
  for ( iTmp1 in 1:nClassVars1 ) {
    deltaTmp1 <- sample(deltaClass1*c(-1,1),iObs,replace=TRUE)
    xyzTmp1[,iTmp1] <- xyzTmp1[,iTmp1] + deltaTmp1
    classTmp1 <- classTmp1 * deltaTmp1
    
  }
  
    classTmp1 <- factor(classTmp1 > 0)
  
    print(table(classTmp1))
# plot resulting attribute levels colored by outcome:
  pairs(xyzTmp1,col=as.numeric(classTmp1))
  
  # Setting up train and test data:
bTrain1 <- sample(c(FALSE,TRUE),nrow(xyzTmp1),replace=TRUE)
bTest1 <- sample(c(FALSE,TRUE),nrow(xyzTmp2),replace=TRUE)
# Fit random forest to train data, obtain test error:
rfRes1 <- randomForest(xyzTmp1[bTrain1,],classTmp1[bTrain1])
rfTmpTbl1 <- table(classTmp2[bTest1],predict(rfRes1,newdata=xyzTmp2[bTest1,]))
print(rfTmpTbl1)

# Fit LDA model to train data and evaluate error on the test data:
ldaRes1 <- lda(xyzTmp1[bTrain1,],classTmp1[bTrain1])
ldaTmpTbl1 <- table(classTmp2[bTest1],predict(ldaRes1,newdata=xyzTmp2[bTest1,])$class)
print(ldaTmpTbl1)

# Fit KNN model at several levels of k:
dfTmp1 <- NULL
  for ( kTmp1 in sort(unique(floor(1.2^(1:15)))) ) {
    knnRes1 <- knn(xyzTmp1[bTrain1,],xyzTmp2[bTest1,],classTmp1[bTrain1],k=kTmp1)
   #knnRes1 <- knn(xyzTmp1[bTrain1,],xyzTmp2[bTest1,],classTmp2[bTest1,],k=kTmp1)
    tmpTbl1 <- table(classTmp2[bTest1],knnRes1)
    dfTmp1 <- rbind(dfTmp1,data.frame(err=1-sum(diag(tmpTbl1))/sum(tmpTbl1),k=kTmp1))
  }
print(ggplot(dfTmp1,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl1))/sum(ldaTmpTbl1),1-sum(diag(rfTmpTbl1))/sum(rfTmpTbl1))))+ggtitle("KNN error rate"))
}

```


We can see that it is the interaction between the first two variables that has influences the outcome (we simulated it this way, of course!) and that points belonging to each of the two classes cannot be readily separated by a single line in 2D (or a single surface in 3D). 
Random forest don't do well on such for 25 observations. Random forest seems to do moderately well on such dataset for 100 and 500 observations. LDA is not so good for 25, 100 and 500 observations, they are 50% accuracy in term of their predictions. When checking KNN, we can see from the above that there is a range of $k$ values where test error of KNN is the lowest and it is even lower than the RF for 100 and 500 observations. The 25 observations provide error rate in between and Random Forest methods. Therefore, it would make sense to choose RF over KNN for modeling the data if the figure above was representative of their true relative performance on a new dataset. Also, at K=8 seems to provide best error rate (lowest error rate). I think with 500 observations, I can use RF method to obtain the minimum test error rate.

# Sub-problem 2 (15 points): effect of signal magnitude

For training datasets with `nObs=100` and `500` observations simulate data as shown above with average differences between the two classes that are same as above, half of that and twice that (i.e. `deltaClass=0.5`, `1` and `2`).  Obtain and plot test error rates of random forest, LDA and KNN for each of the six (two samples sizes times three signal magnitudes) combinations of sample size and signal strengths.  As before use large test dataset (e.g. 10K observations or so) for greater stability of the results.  Describe the most pronounced differences across error rates for those datasets: does the increase in the number of observations impact the error rate of the models?  Does change in the magnitude of signal impact their performance?  Are different classifier approaches impacted in a similar way?

Answer: 
```{r}
# How many observations:
nObs3 <- c(100, 500)
# How many predictors are associated with outcome:
nClassVars3 <- 2
# How many predictors are not:
nNoiseVars3 <- 3
# To modulate average difference between two classes' predictor values:
deltaClass3 <- c(0.5,1,2)
# Simulate dataset with interaction between attribute levels associated with the outcome:

for (iObs in nObs3) { 
  for (ideltaClass3 in deltaClass3) {
  xyzTmp3 <- matrix(rnorm(iObs*(nClassVars3+nNoiseVars3)),nrow=iObs,ncol=nClassVars3+nNoiseVars3)
  classTmp3 <- 1
  for ( iTmp3 in 1:nClassVars3 ) {
    deltaTmp3 <- sample(ideltaClass3*c(-1,1),iObs,replace=TRUE)
    xyzTmp3[,iTmp3] <- xyzTmp3[,iTmp3] + deltaTmp3
    classTmp3 <- classTmp3 * deltaTmp3
    }
  classTmp3 <- factor(classTmp3 > 0)
  print(table(classTmp3))
# plot resulting attribute levels colored by outcome:
  pairs(xyzTmp3,col=as.numeric(classTmp3))

# Split data into train and test
bTrain2 <- sample(c(FALSE,TRUE),nrow(xyzTmp3),replace=TRUE)

# Fit random forest to train data, obtain test error:
rfRes2 <- randomForest(xyzTmp3[bTrain2,],classTmp3[bTrain2])
rfTmpTbl2 <- table(classTmp2[bTest1],predict(rfRes2,newdata=xyzTmp2[bTest1,]))
print(rfTmpTbl2)

# Fit LDA model to train data and evaluate error on the test data:
ldaRes2 <- lda(xyzTmp3[bTrain2,],classTmp3[bTrain2])
ldaTmpTbl2 <- table(classTmp2[bTest1],predict(ldaRes2,newdata=xyzTmp2[bTest1,])$class)
print(ldaTmpTbl2)

# Fit KNN model at several levels of k:
dfTmp2 <- NULL
  for ( kTmp2 in sort(unique(floor(1.2^(1:15)))) ) {
    knnRes2 <- knn(xyzTmp3[bTrain2,],xyzTmp2[bTest1,],classTmp3[bTrain2],k=kTmp2)
    tmpTbl2 <- table(classTmp2[bTest1],knnRes2)
    dfTmp2 <- rbind(dfTmp2,data.frame(err=1-sum(diag(tmpTbl2))/sum(tmpTbl2),k=kTmp2))
  }
print(ggplot(dfTmp2,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl2))/sum(ldaTmpTbl2),1-sum(diag(rfTmpTbl2))/sum(rfTmpTbl2))))+ggtitle("KNN error rate"))
  }
}
```


For training datasets with `nObs=100` and `500` observations simulate data as shown above with average differences between the two classes that are same as above, half of that and twice that (i.e. `deltaClass=0.5`, `1` and `2`). Test error rates of random forest were obtained and plotted, LDA and KNN for each of the six (two samples sizes times three signal magnitudes) combinations of sample size and signal strengths.
For these specific datasets, Random forest method provides 50% accuracy, LDA is not so good for prediction. 
When checking KNN, we can see from the above that there is a range of $k$ values where test error of KNN is the lowest and it is even lower than the RF for 100 and 500 observations. With respect of deltaClass, with 500 observations and overlap between 2 classess = 1 provide the ideal error rate. Overall, the increase in the number of observations strongly impact the error rate of the models.  Change in the magnitude of signal impact their performance and different classifier approaches are impacted in a different way.


# Sub-problem 3 (15 points): varying counts of predictors

For all possible pairwise combinations of the numbers of variables associated with outcome (`nClassVars=2` and `5`) and those not associated with the outcome (`nNoiseVars=1`, `3` and `10`) -- six pairwise combinations in total -- obtain and present graphically test errors from random forest, LDA and KNN.  Choose signal magnitude (`deltaClass`) and training data sample size so that this simulation yields non-trivial results -- noticeable variability in the error rates across those six pairwise combinations of attribute counts.  Describe the results: what is the impact of the increase of the number of attributes associated with the outcome on the classifier performance?  What about the number of attributes not associated with outcome - does it affect classifier error rate?  Are different classifier methods affected by these simulation parameters in a similar way?
Answer:
```{r}
# How many observations:
nObs4 <- 500
# How many predictors are associated with outcome:
nClassVars4 <- c(2,5)
# How many predictors are not:
nNoiseVars4 <- c(1,3,10)
# To modulate average difference between two classes' predictor values:
deltaClass4 <- 1
# Simulate dataset with interaction between attribute levels associated with the outcome:

for (iClassvars in nClassVars4) { 
  for (iNoiseVars in nNoiseVars4) {
  xyzTmp4 <- matrix(rnorm(nObs4*(iClassvars+iNoiseVars)),nrow=nObs4,ncol=iClassvars+iNoiseVars)
  classTmp4 <- 1
  for ( iTmp4 in 1:nClassVars4 ) {
    deltaTmp4 <- sample(deltaClass4*c(-1,1),nObs4,replace=TRUE)
    xyzTmp4[,iTmp4] <- xyzTmp4[,iTmp4] + deltaTmp4
    classTmp4 <- classTmp4 * deltaTmp4
    }
    classTmp4 <- factor(classTmp4 > 0)
    print(table(classTmp4))
# plot resulting attribute levels colored by outcome:
  pairs(xyzTmp4,col=as.numeric(classTmp4))
# Split data into train and test
bTrain3 <- sample(c(FALSE,TRUE),nrow(xyzTmp4),replace=TRUE)
# Fit random forest to train data, obtain test error:
rfRes3 <- randomForest(xyzTmp4[bTrain3,],classTmp4[bTrain3])
rfTmpTbl3 <- table(classTmp4[!bTrain3],predict(rfRes3,newdata=xyzTmp4[!bTrain3,]))
print(rfTmpTbl3)

# Fit LDA model to train data and evaluate error on the test data:
ldaRes3 <- lda(xyzTmp4[bTrain3,],classTmp4[bTrain3])
ldaTmpTbl3 <- table(classTmp4[!bTrain3],predict(ldaRes3,newdata=xyzTmp4[!bTrain3,])$class)
print(ldaTmpTbl3)

# Fit KNN model at several levels of k:
dfTmp3 <- NULL
for ( kTmp3 in sort(unique(floor(1.2^(1:15)))) ) {
  knnRes3 <- knn(xyzTmp4[bTrain3,],xyzTmp4[!bTrain3,],classTmp4[bTrain3],k=kTmp3)
  tmpTbl3 <- table(classTmp4[!bTrain3],knnRes3)
  dfTmp3 <- rbind(dfTmp3,data.frame(err=1-sum(diag(tmpTbl3))/sum(tmpTbl3),k=kTmp3))
}
print(ggplot(dfTmp3,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl3))/sum(ldaTmpTbl3),1-sum(diag(rfTmpTbl3))/sum(rfTmpTbl3))))+ggtitle("KNN error rate"))
  }
}
```


Looking at the above graphs, for all possible pairwise combinations of the numbers of variables associated with outcome (`nClassVars=2` and `5`) and those not associated with the outcome (`nNoiseVars=1`, `3` and `10`) -- six pairwise combinations in total -- obtain and present graphically test errors from random forest, LDA and KNN.  I chose signal magnitude (`deltaClass` = 1 as known as 1 class overlap between 2 groups) and training data sample size 500 so that this simulation yields non-trivial results -- noticeable variability in the error rates across those six pairwise combinations of attribute counts.  

From what see, increasing of the number of attributes associated with the outcome on the classifier performance helps stablizing the cluster number provide consistentminimum error rate.  The number of attributes not associated with outcome affects classifier error rate in a negative way as it increases.  Different classifier methods are affected by these simulation parameters in an opposite way.
# Sub-problem 4: (15 points): effect of `mtry`

Parameter `mtry` in the call to `randomForest` defines the number of predictors randomly chosen to be evaluated for their association with the outcome at each split (please see help page for `randomForest` for more details).  By default for classification problem it is set as a square root of the number of predictors in the dataset.  Here we will evaluate the impact of using different values of `mtry` on the error rate by random forest.

For `nObs=5000`, `deltaClass=2`, `nClassVars=3` and `nNoiseVars=20` generate data using the above approach, run `randomForest` on it with `mtry=2`, `5` and `10` and obtain corresponding test error for these three models.  Describe the impact of using different values of `mtry` on the test error rate by random forest and compare it to that by LDA/KNN. 
Answer: First, the requested data is generated:
```{r}
# How many observations:
nObs5 <- 5000
# How many predictors are associated with outcome:
nClassVars5 <- 3
# How many predictors are not:
nNoiseVars5 <- 20
# To modulate average difference between two classes' predictor values:
deltaClass5 <- 2
# Simulate dataset with interaction between attribute levels associated with the outcome:
xyzTmp5 <- matrix(rnorm(nObs5*(nClassVars5+nNoiseVars5)),nrow=nObs5,ncol=nClassVars5+nNoiseVars5)
classTmp5 <- 1
for ( iTmp in 1:nClassVars5 ) {
  deltaTmp5 <- sample(deltaClass5*c(-1,1),nObs5,replace=TRUE)
  xyzTmp5[,iTmp] <- xyzTmp5[,iTmp] + deltaTmp5
  classTmp5 <- classTmp5 * deltaTmp5
}
classTmp5 <- factor(classTmp5 > 0)
table(classTmp5)
# plot resulting attribute levels colored by outcome:
pairs(xyzTmp5,col=as.numeric(classTmp5))
```


```{r}
Mtry=c(2,5,10)
# Split data into train and test
bTrain4 <- sample(c(FALSE,TRUE),nrow(xyzTmp5),replace=TRUE)
# Fit random forest to train data, obtain test error:
for (imtry in Mtry) {
rfRes4 <- randomForest(xyzTmp5[bTrain4,],classTmp5[bTrain4], mtry=imtry)
rfTmpTbl4 <- table(classTmp5[!bTrain4],predict(rfRes4,newdata=xyzTmp5[!bTrain4,]))
print(rfTmpTbl4)

# Fit LDA model to train data and evaluate error on the test data:
ldaRes4 <- lda(xyzTmp5[bTrain4,],classTmp5[bTrain4])
ldaTmpTbl4 <- table(classTmp5[!bTrain4],predict(ldaRes4,newdata=xyzTmp5[!bTrain4,])$class)
print(ldaTmpTbl4)

# Fit KNN model at several levels of k:
dfTmp4 <- NULL
for ( kTmp4 in sort(unique(floor(1.2^(1:15)))) ) {
  knnRes4 <- knn(xyzTmp5[bTrain4,],xyzTmp5[!bTrain4,],classTmp5[bTrain4],k=kTmp4)
  tmpTbl4 <- table(classTmp5[!bTrain4],knnRes4)
  dfTmp4 <- rbind(dfTmp4,data.frame(err=1-sum(diag(tmpTbl4))/sum(tmpTbl4),k=kTmp4))
  }
print(ggplot(dfTmp4,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl4))/sum(ldaTmpTbl4),1-sum(diag(rfTmpTbl4))/sum(rfTmpTbl4))))+ggtitle("KNN error rate"))
}
```


According to R, mtry represents the number of variables randomly sampled as candidates at each split. Note that
the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3). In this case, the impact of using different values of `mtry` (mtry=1,5,10) on the test error rate by random forest provides similar error rate comparing to LDA/KNN. 
Good luck to me!